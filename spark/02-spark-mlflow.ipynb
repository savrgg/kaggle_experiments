{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/27 12:14:29 WARN Utils: Your hostname, Salvadors-iMac.local resolves to a loopback address: 127.0.0.1; using 192.168.100.97 instead (on interface en1)\n",
      "23/10/27 12:14:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/27 12:14:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/10/27 12:14:35 WARN Instrumentation: [07826b3e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/10/27 12:14:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/10/27 12:14:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/10/27 12:14:35 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/10/27 12:14:46 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "/Users/salvadorgarcia/Repos/kaggle_experiments/spark/.venv/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 5.063396036227354e-15\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import os\n",
    "\n",
    "\n",
    "def train_linear_regression_model(data, train_fraction=0.8):\n",
    "    # Create a Spark session\n",
    "    spark = SparkSession.builder.appName(\"LinearRegressionExample\").getOrCreate()\n",
    "\n",
    "    # Specify the MLflow YAML configuration file\n",
    "    # mlflow_config_file = \"./mlflow.yaml\"\n",
    "\n",
    "    # Load the MLflow configuration from the YAML file\n",
    "    # mlflow.set_tracking_uri(\"file://{}\".format(mlflow_config_file))\n",
    "\n",
    "    # Create an MLflow run\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"train_fraction\", train_fraction)\n",
    "        \n",
    "        # Create a DataFrame with the correct data type\n",
    "        df = spark.createDataFrame(data, [\"label\", \"features\"]).withColumn(\"features\", col(\"features\").cast(VectorUDT()))\n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_data, test_data = df.randomSplit([train_fraction, 1 - train_fraction], seed=123)\n",
    "\n",
    "        # Create a LinearRegression model\n",
    "        lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "        # Fit the model to the training data\n",
    "        lr_model = lr.fit(train_data)\n",
    "\n",
    "        # Log model parameters\n",
    "        mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "\n",
    "        # Log the model itself\n",
    "        mlflow.spark.log_model(lr_model, \"model\")\n",
    "\n",
    "        return lr_model, test_data, spark\n",
    "\n",
    "def score_linear_regression_model(model, test_data,spark):\n",
    "    # Make predictions on the test data\n",
    "    predictions = model.transform(test_data)\n",
    "    return predictions\n",
    "\n",
    "def evaluate_linear_regression_model(predictions):\n",
    "    # Check if there are any predictions\n",
    "    if predictions.count() > 0:\n",
    "        # Evaluate the model\n",
    "        evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Log RMSE as a metric\n",
    "        mlflow.log_metric(\"RMSE\", rmse)\n",
    "\n",
    "        return rmse\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def main():\n",
    "    # Create a larger synthetic dataset with 50 data points\n",
    "    data = [(1.0, Vectors.dense(0.1, 0.2)),\n",
    "            (2.0, Vectors.dense(0.2, 0.3)),\n",
    "            (3.0, Vectors.dense(0.3, 0.4)),\n",
    "            (4.0, Vectors.dense(0.4, 0.5)),\n",
    "            (5.0, Vectors.dense(0.5, 0.6)),\n",
    "            (6.0, Vectors.dense(0.6, 0.7)),\n",
    "            (7.0, Vectors.dense(0.7, 0.8)),\n",
    "            (8.0, Vectors.dense(0.8, 0.9)),\n",
    "            (9.0, Vectors.dense(0.9, 1.0)),\n",
    "            (10.0, Vectors.dense(1.0, 1.1)),\n",
    "            (11.0, Vectors.dense(1.1, 1.2)),\n",
    "            (12.0, Vectors.dense(1.2, 1.3)),\n",
    "            (13.0, Vectors.dense(1.3, 1.4)),\n",
    "            (14.0, Vectors.dense(1.4, 1.5)),\n",
    "            (15.0, Vectors.dense(1.5, 1.6)),\n",
    "            (16.0, Vectors.dense(1.6, 1.7)),\n",
    "            (17.0, Vectors.dense(1.7, 1.8)),\n",
    "            (18.0, Vectors.dense(1.8, 1.9)),\n",
    "            (19.0, Vectors.dense(1.9, 2.0)),\n",
    "            (20.0, Vectors.dense(2.0, 2.1)),\n",
    "            (21.0, Vectors.dense(2.1, 2.2)),\n",
    "            (22.0, Vectors.dense(2.2, 2.3)),\n",
    "            (23.0, Vectors.dense(2.3, 2.4)),\n",
    "            (24.0, Vectors.dense(2.4, 2.5)),\n",
    "            (25.0, Vectors.dense(2.5, 2.6)),\n",
    "            (26.0, Vectors.dense(2.6, 2.7)),\n",
    "            (27.0, Vectors.dense(2.7, 2.8)),\n",
    "            (28.0, Vectors.dense(2.8, 2.9)),\n",
    "            (29.0, Vectors.dense(2.9, 3.0)),\n",
    "            (30.0, Vectors.dense(3.0, 3.1)),\n",
    "            (31.0, Vectors.dense(3.1, 3.2)),\n",
    "            (32.0, Vectors.dense(3.2, 3.3)),\n",
    "            (33.0, Vectors.dense(3.3, 3.4)),\n",
    "            (34.0, Vectors.dense(3.4, 3.5)),\n",
    "            (35.0, Vectors.dense(3.5, 3.6)),\n",
    "            (36.0, Vectors.dense(3.6, 3.7)),\n",
    "            (37.0, Vectors.dense(3.7, 3.8)),\n",
    "            (38.0, Vectors.dense(3.8, 3.9)),\n",
    "            (39.0, Vectors.dense(3.9, 4.0)),\n",
    "            (40.0, Vectors.dense(4.0, 4.1)),\n",
    "            (41.0, Vectors.dense(4.1, 4.2)),\n",
    "            (42.0, Vectors.dense(4.2, 4.3)),\n",
    "            (43.0, Vectors.dense(4.3, 4.4)),\n",
    "            (44.0, Vectors.dense(4.4, 4.5)),\n",
    "            (45.0, Vectors.dense(4.5, 4.6)),\n",
    "            (46.0, Vectors.dense(4.6, 4.7)),\n",
    "            (47.0, Vectors.dense(4.7, 4.8)),\n",
    "            (48.0, Vectors.dense(4.8, 4.9)),\n",
    "            (49.0, Vectors.dense(4.9, 5.0)),\n",
    "            (50.0, Vectors.dense(5.0, 5.1))]\n",
    "\n",
    "    # Train the Linear Regression model\n",
    "    lr_model, test_data, spark = train_linear_regression_model(data, train_fraction=0.8)\n",
    "\n",
    "    # Score the model on the test data\n",
    "    predictions = score_linear_regression_model(lr_model, test_data, spark)\n",
    "\n",
    "    # Evaluate the model\n",
    "    rmse = evaluate_linear_regression_model(predictions)\n",
    "\n",
    "    if rmse is not None:\n",
    "        print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    else:\n",
    "        print(\"No predictions were made. Check your data or model.\")\n",
    "\n",
    "    # End the MLflow run\n",
    "    mlflow.end_run()\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mlflow server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
